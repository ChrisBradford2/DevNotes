---
id: lintegration-et-la-livraison-continues-avec-la-demarche-devops
title: L'intÃ©gration et la livraison continues avec la dÃ©marche DevOps
description: My document description
slug: /lintegration-et-la-livraison-continues-avec-la-demarche-devops
---

# L'intÃ©gration et la livraison continues avec la dÃ©marche DevOps

## Qu'est-ce que l'intÃ©gration continue ?

> **L'intÃ©gration et la livraison continues**, ou en anglais _**Continuous Integration and Continuous Delivery (CI/CD)**_ permettent de :
> - **accÃ©lÃ©rer le _Time-to-Market_** (le temps de dÃ©veloppement et de mise en production d'une fonctionnalitÃ©) ;
> - **rÃ©duire les erreurs** lors des livraisons ;
> - assurer une **continuitÃ© de service** des applications.

**L'intÃ©gration continue** est un ensemble de pratiques utilisÃ©es en gÃ©nie logiciel, consistant Ã  vÃ©rifier, Ã  chaque modification de code source que le rÃ©sultat des modifications ne produit pas de rÃ©gression dans l'application dÃ©veloppÃ©e.

Le principe de l'intÃ©gration continue est justement de dÃ©tecter ces problÃ¨mes d'intÃ©gration au plus tÃ´t dans le cycle de dÃ©veloppement.

> L'intÃ©gration continue va se faire en 5 Ã©tapes :
> 1. Planifiez votre dÃ©veloppement.
> 2. Compilez et intÃ©grez votre code.
> 3. Testez votre code.
> 4. Mesurez la qualitÃ© de votre code.
> 5. GÃ©rez les livrables de votre application.

Toutes les Ã©tapes se feront sur GitLab. Les Ã©tapes 2 Ã  4 seront lancÃ©es automatiquement grÃ¢ce Ã  GitLab CI :

![Ã‰tapes de l'intÃ©gration continue sur GitLab](https://user.oc-static.com/upload/2019/05/23/15586109893555_1c3_illus_WHITEBG%20%280-00-17-23%29_4.png)

### Ã‰tape 1 : Planifiez votre dÃ©veloppement

Afin de **savoir quoi dÃ©velopper**, il est nÃ©cessaire d'avoir Ã  disposition un outil permettant la collaboration entre les dÃ©veloppeurs. Cet outil permettra notamment de gÃ©rer les diffÃ©rentes releases et toutes les fonctionnalitÃ©s, de garantir la prioritÃ© du backlog, etc.

Intervenant tout au long du projet, la collaboration de toute l'Ã©quipe est nÃ©cessaire pour assurer la planification du projet. Cette planification est Ã©troitement liÃ©e Ã  la mÃ©thodologie Scrum. Elle a pour but de dÃ©couper le projet en petites tÃ¢ches Ã  rÃ©aliser par toute l'Ã©quipe.

> Pour collaborer avec vos Ã©quipes, vous pourrez utiliser **Jira**, **GitLab**, **Confluence**, **ALM Octane** ou encore **Pivotal Tracker**.

### Ã‰tape 2 : Compilez et intÃ©grez votre code

#### Le contrÃ´le de code source

Le code source se doit d'Ãªtre disponible Ã  chaque instant sur un dÃ©pÃ´t central. Chaque dÃ©veloppement doit faire l'objet d'un suivi de rÃ©vision. Le code doit Ãªtre compilable Ã  partir d'une rÃ©cupÃ©ration fraÃ®che, et ne faire l'objet d'aucune dÃ©pendance externe. MÃªme s'il existe des notions de branche, la crÃ©ation d'une branche doit Ãªtre Ã©vitÃ©e le plus possible, privilÃ©giant le dÃ©veloppement sur la branche principale ; cela Ã©vite de maintenir plusieurs versions en parallÃ¨le. Ce genre de pratique est appelÃ© *trunk-based development*.

> Pour faire du contrÃ´le de source, vous retrouverez les outils **Git**, **Subversion**, **GitHub**, **GitLab**, **Perforce** ou bien **Bitbucket**.

#### L'orchestrateur

Ensuite, toutes les Ã©tapes doivent Ãªtre automatisÃ©es par un **orchestrateur**, qui saura reproduire ces Ã©tapes et gÃ©rer les dÃ©pendances entre elles. De plus, l'utilisation d'un orchestrateur permet de donner accÃ¨s Ã  tous, et Ã  tout moment, Ã  un tableau de bord qui donnera l'Ã©tat de santÃ© des Ã©tapes d'intÃ©gration continue. Ainsi, les dÃ©veloppeurs ont au plus tÃ´t la boucle de feedback nÃ©cessaire, afin de garantir que l'application soit prÃªte Ã  tout moment. De plus, l'orchestrateur permettra d'aller plus loin dans la livraison continue.

> L'orchestration des Ã©tapes de votre intÃ©gration continue peut se faire grÃ¢ce Ã  des outils comme **Jenkins**, **TeamCity**, **Azure DevOps**, **GitLab CI**, **Concours CI**, **Travis CI** ou **Bamboo**.

La premiÃ¨re Ã©tape, et celle qui paraÃ®t la plus Ã©vidente, est de compiler le code de maniÃ¨re continue. En effet, sans cette Ã©tape, le code est compilÃ© manuellement sur le poste du dÃ©veloppeur, afin que ce dernier s'assure que son code compile.

Malheureusement, comme dit prÃ©cÃ©demment, le dÃ©veloppeur ne s'assure pas que son code permet de bien compiler avec tous les autres dÃ©veloppements faits par l'Ã©quipe. Ã€ la prochaine livraison, un dÃ©veloppeur intÃ¨gre alors manuellement toutes les modifications, une opÃ©ration qui produit beaucoup de peine et de souffrance.

La mise en place d'une premiÃ¨re Ã©tape de compilation dans un processus d'intÃ©gration continue permet justement de ne plus se soucier si des modifications de code cassent la compilation. Le dÃ©veloppeur doit alors s'assurer de bien envoyer son code source sur le dÃ©pÃ´t central. En faisant cela, il dÃ©clenche une premiÃ¨re Ã©tape de compilation, avec toutes les modifications des autres dÃ©veloppeurs. Si la compilation ne se fait pas, le code est alors rejetÃ©, et le dÃ©veloppeur doit corriger ses erreurs.

AprÃ¨s cette premiÃ¨re Ã©tape, le code devient plus sÃ»r, et le dÃ©pÃ´t de code source garantit qu'Ã  chaque instant, un dÃ©veloppeur rÃ©cupÃ¨re un code qui compile. Dans cette Ã©tape, les tests ne sont pas encore exÃ©cutÃ©s. Le code peut donc Ãªtre de mauvaise qualitÃ©.

> Vous pourrez compiler votre code avec **Maven**, **Ant**, **Gradle**, **MSBuild**, **NAnt**, **Gulp** ou encore **Grunt**.

### Ã‰tape 3 : Testez votre code

#### Les tests unitaires

Dans cette Ã©tape, **l'orchestrateur se charge de lancer les tests unitaires Ã  la suite de la compilation**. Ces tests unitaires, gÃ©nÃ©ralement avec un framework associÃ©, garantissent que le code respecte un certain niveau de qualitÃ©.

> Les tests unitaires permettent de vÃ©rifier le bon fonctionnement d'une partie prÃ©cise d'un logiciel ou d'une portion d'un programme.

Plus il y a de tests unitaires, plus le code est garanti sÃ»r. Ã‰videmment, l'orchestrateur ne peut lancer que les tests qui ont Ã©tÃ© codÃ©s par les dÃ©veloppeurs, et ne peut pas inventer de nouveaux cas de tests.

Ces tests doivent s'exÃ©cuter **de la maniÃ¨re la plus rapide possible**, afin d'avoir un feedback le plus rapide lui aussi. Pour arriver Ã  ce niveau, **il est nÃ©cessaire que les tests unitaires n'aient aucune dÃ©pendance** vis-Ã -vis de systÃ¨mes externes, comme par exemple une base de donnÃ©es, ou mÃªme le systÃ¨me de fichiers de la machine.

Les tests unitaires apportent 3 atouts Ã  la production :
- **trouver les erreurs plus facilement**. Les tests sont exÃ©cutÃ©s durant tout le dÃ©veloppement, permettant de visualiser si le code fraÃ®chement Ã©crit correspond au besoin ;
- **sÃ©curiser la maintenance**. Lors d'une modification d'un programme, les tests unitaires signalent les Ã©ventuelles rÃ©gressions. En effet, certains tests peuvent Ã©chouer Ã  la suite d'une modification, il faut donc soit rÃ©Ã©crire le test pour le faire correspondre aux nouvelles attentes, soit corriger l'erreur se situant dans le code ;
- **documenter le code**. Les tests unitaires peuvent servir de complÃ©ment Ã  la documentation ; il est trÃ¨s utile de lire les tests pour comprendre comment s'utilise une mÃ©thode. De plus, il est possible que la documentation ne soit plus Ã  jour, mais les tests, eux, correspondent Ã  la rÃ©alitÃ© de l'application.

**L'ensemble des tests unitaires doivent Ãªtre relancÃ©s aprÃ¨s une modification** du code, afin de vÃ©rifier qu'il n'y ait pas de rÃ©gressions (l'apparition de nouveaux dysfonctionnements). 

La multiplicitÃ© des test unitaires oblige Ã  les **maintenir** dans le temps, au fur et Ã  mesure que le dÃ©veloppement avance.

> Pour implÃ©menter et exÃ©cuter vos tests unitaires, vous retrouverez des outils comme **JUnit**, **NUnit** ou encore **XUnit**.

### Ã‰tape 4 : Mesurez la qualitÃ© de votre code

Maintenant que les tests unitaires sont Ã©crits et exÃ©cutÃ©s, nous commenÃ§ons Ã  avoir une meilleure qualitÃ© de code, et Ã  Ãªtre rassurÃ©s sur la fiabilitÃ© et la robustesse de l'application. GrÃ¢ce Ã  la compilation et aux tests unitaires, nous pouvons maintenant **mesurer la qualitÃ© du code**. Tout ceci permet aux dÃ©veloppeurs de **maintenir** dans le temps un code de trÃ¨s bonne qualitÃ©, alertant l'Ã©quipe en cas de dÃ©rive des bonnes pratiques de tests.

> L'Ã©tape de qualitÃ© de code est diffÃ©rente de l'Ã©tape de test, car cette Ã©tape de **qualitÃ©** assure que le code sera **maintenable** et **Ã©volutif** au fur et Ã  mesure de son cycle de vie, alors que les **tests** servent Ã  garantir que le code **implÃ©mente bien les fonctionnalitÃ©s** demandÃ©es, et ne contient pas (ou peu) de **bugs**.

Lors de l'Ã©tape de qualitÃ© de code, nous cherchons Ã  assurer la plus petite **dette technique** possible de notre application. La dette technique est le temps nÃ©cessaire Ã  la correction de bugs ou Ã  l'ajout de nouvelles fonctionnalitÃ©s, lorsque nous ne respectons pas les rÃ¨gles de coding. La dette est exprimÃ©e en **heures de correction**. Plus cette dette est Ã©levÃ©e, plus le code sera difficile Ã  maintenir et Ã  faire Ã©voluer.

L'Ã©tape de qualitÃ© de code mesure aussi d'autres mÃ©triques, comme le nombre de vulnÃ©rabilitÃ©s au sein du code, la couverture de test, mais aussi les [*code smells*](https://fr.wikipedia.org/wiki/Code_smell) (qui sont des mauvaises pratiques Ã  ne pas implÃ©menter), la [complexitÃ© cyclomatique](https://fr.wikipedia.org/wiki/Nombre_cyclomatique) (complexitÃ© du code applicatif) ou la duplication de code. C'est le rÃ´le du dÃ©veloppeur de respecter les normes dÃ©finies et de corriger au fur et Ã  mesure son code.

Afin de renforcer la qualitÃ© du code et de ne pas autoriser le dÃ©ploiement d'un code de mauvaise qualitÃ©, nous pouvons implÃ©menter un **arrÃªt complet du pipeline d'intÃ©gration continue, si le code n'atteint pas la qualitÃ© requise**.

> **Les outils** : la qualitÃ© du code peut Ãªtre Ã©valuÃ©e grÃ¢ce Ã  **SonarQube**, **Cast** ou **GitLab Code Quality**.

### Ã‰tape 5 : GÃ©rez les livrables de votre application

Le code, une fois compilÃ©, doit Ãªtre dÃ©ployÃ© dans un dÃ©pÃ´t de livrables, et versionnÃ©. Les binaires produits sont appelÃ©s **_artefacts_**. Ces artefacts doivent Ãªtre accessibles Ã  toutes les parties prenantes de l'application, afin de pouvoir les dÃ©ployer et lancer les tests autres qu'unitaires (test de performance, test de bout en bout, etc.). Ces artefacts sont disponibles dans un stockage, centralisÃ© et organisÃ©, de donnÃ©es. Ce peut Ãªtre une ou plusieurs bases de donnÃ©es oÃ¹ les artefacts sont localisÃ©s en vue de leur distribution sur le rÃ©seau, ou bien un endroit directement accessible aux utilisateurs.

> La mise Ã  disposition des artefacts peut Ãªtre faite par **Nexus**, **Artifactory**, **GitLab repository**, **Quay**, **Docker Hub**.

### Utilisez GitLab pour mettre en place un pipeline CI/CD

Dans la suite de ce cours, nous ferons le choix de GitLab. Cet outil a l'avantage d'avoir toutes les briques nÃ©cessaires Ã  la mise en place de l'intÃ©gration continue, sans rentrer dans des Ã©tapes complexes de mise en place des outils, ainsi que les connexions associÃ©es.

Afin de pouvoir illustrer l'intÃ©gration continue, nous allons travailler sur un projet open source : la mise en place du site web d'une clinique vÃ©tÃ©rinaire via Spring Boot. Toutes les Ã©tapes de mise en place de l'intÃ©gration continue seront entiÃ¨rement dÃ©taillÃ©es dans la suite de cette partie.

> Vous n'avez pas besoin de connaÃ®tre Spring Boot, ni d'Ãªtre calÃ© en dÃ©veloppement Java. Tout le code vous sera fourni, vous lui ferez passer toutes les Ã©tapes du pipeline CI/CD.

## Planifiez votre dÃ©veloppement

> La mÃ©thodologie DevOps emprunte de nombreux concepts Ã  **Scrum**. Afin de bien suivre la suite de ce chapitre, il vous sera donc nÃ©cessaire d'avoir des connaissances du vocabulaire et de la mÃ©thodologie Scrum. Il vous faudra notamment connaÃ®tre les termes *epic*, *user story*, *task*, *bug*, *sprint*, *Product Backlog*, *board*,*burndown chart*, *Definition of Done*, *Definition of Ready*, tels que dÃ©finis par la mÃ©thodologie Scrum.

Dans GitLab, l'**issue** est la notion centrale pour dÃ©finir n'importe quoi : que ce soit une epic, une feature ou mÃªme un bug. La distinction se fait via des labels, que l'on positionne sur l'issue.

Il est temps de dÃ©finir notre premier **backlog**, c'est-Ã -dire l'ensemble des tÃ¢ches Ã  rÃ©aliser sur notre projet.

### CrÃ©ez votre projet GitLab

Dans un premier temps, nous allons **crÃ©er un projet dans GitLab**. Pour cela, nous allons utiliser la plateforme [https://gitlab.com/](https://gitlab.com/), afin d'Ã©viter d'installer notre propre plateforme GitLab en local.

Si vous n'avez pas encore de compte sur GitLab, je vous invite Ã  vous en crÃ©er un, puis Ã  vous connecter.

Une fois connectÃ©, vous arrivez sur une page listant tous vos projets. Si vous venez de crÃ©er le compte, normalement cette page sera vide.

> Comme expliquÃ© dans le premier chapitre, nous allons nous baser sur le projet **PetClinic** tout au long du cours. Dans ce chapitre, nous allons commencer Ã  planifier le dÃ©veloppement de fonctionnalitÃ©s pour ce projet.

Pour **crÃ©er un nouveau projet dans GitLab**, une fois connectÃ©, cliquez sur le bouton `New Project`. Sur la nouvelle page, il faut entrer un nouveau nom de projet. Dans le champ Project Name, nous allons entrer le nom **spring-petclinic-microservices**. Notez au passage que GitLab remplit automatiquement le champ Project Slug. Il est impÃ©ratif de passer le projet en **Public** afin de bÃ©nÃ©ficier de toutes les fonctionnalitÃ©s abordÃ©es dans ce cours. Une fois tous les champs remplis, l'Ã©cran devrait ressembler Ã  cela :

![CrÃ©ation d'un projet sur Gitlab](https://user.oc-static.com/upload/2019/05/23/15586242595092_new-project-detail.png)

Appuyez maintenant sur le bouton *Create Project*. Vous arrivez alors sur la page du projet fraÃ®chement crÃ©Ã©. Avant de rÃ©cupÃ©rer le code source de l'application, et de l'intÃ©grer dans GitLab, nous allons mettre en place les diffÃ©rentes briques nÃ©cessaires au bon dÃ©roulement du projet. Tout d'abord, nous allons naviguer dans la partie Issues, afin d'ajouter les colonnes nÃ©cessaires Ã  notre projet. Pour ce faire, il suffit d'aller sur le menu Ã  gauche, survoler le menu Issues, et cliquer sur le lien Boards. Sur cette page, GitLab nous propose d'ajouter les listes par dÃ©faut via le bouton Add default lists. GitLab crÃ©e alors deux nouvelles colonnes, To Do et Doing :

![Votre board sur GitLab](https://user.oc-static.com/upload/2019/05/23/15586243172078_boards-new.png)

Ce **board** sera notre board par dÃ©faut durant tout notre projet, afin de voir son avancement. Nous allons ensuite **crÃ©er les diffÃ©rents labels** que nous avons vus prÃ©cÃ©demment, afin de pouvoir crÃ©er et catÃ©goriser les issues que nous allons ouvrir. Pour ce faire, nous allons aller dans le sous-menu Labels du menu Issues. Sur la nouvelle page, il faut alors cliquer sur le bouton **New Label** pour crÃ©er une nouvelle catÃ©gorie d'issue. Notez que GitLab a dÃ©jÃ  crÃ©Ã© les deux labels To Do et Doing pour nous.

Sur la page de crÃ©ation de nouveaux labels, nous allons ajouter tous les labels nÃ©cessaires Ã  la catÃ©gorisation des issues :

![CrÃ©ation d'un nouveau label](https://user.oc-static.com/upload/2019/05/23/15586244055298_label-new.png)

> **Les catÃ©gories Ã  crÃ©er sont** : Epic, User Story, Bug, Ready, Rejected, High, Medium, Low, In Review. Une fois les labels crÃ©Ã©s, la liste devrait ressembler Ã  ceci :

![Tous vos labels sont prÃªts !](https://user.oc-static.com/upload/2019/05/23/15586244563257_labels-list.png)

Vous noterez la crÃ©ation de plusieurs labels que je n'ai pas dÃ©taillÃ©s : High, Medium, Low, Ready, Rejected et In Review. Ces labels sont utilisÃ©s pour catÃ©goriser plusieurs issues et pour crÃ©er un nouveau board, que nous allons appeler **Product Backlog**. Pour ce faire, il faut retourner dans le menu Board. Tout d'abord, dans la page courante, cliquez sur Add List et sÃ©lectionnez le label In Review pour ajouter une nouvelle colonne dans le board courant. Le nouveau board doit ressembler Ã  cela (pour des questions de lisibilitÃ©, j'ai repliÃ© les colonnes Open et Closed dans la capture d'Ã©cran suivante) :

![Nouvelle colonne sur votre board de dÃ©veloppement](https://user.oc-static.com/upload/2019/05/23/15586245146433_development-board.png)

Ensuite, sur la page, en haut Ã  gauche, il y a une liste dÃ©roulante contenant tous les boards crÃ©Ã©s. Pour l'instant, il n'y en a qu'un seul, celui de dÃ©veloppement qui est actuellement affichÃ©.

Dans cette liste dÃ©roulante, sÃ©lectionnez **Create New Board**, puis donnez-lui le nom de **Product**. Ce board sera notre **Product Backlog** qui listera toutes les epics, ainsi que les user stories associÃ©es et leurs Ã©tats respectifs :

![CrÃ©ez votre Product Backlog](https://user.oc-static.com/upload/2019/05/23/1558624642716_new-board.png)

Sur la nouvelle page, aprÃ¨s avoir cliquÃ© sur le bouton Create Board, il faut maintenant ajouter les colonnes Low, Medium, High et Rejected afin de pouvoir classifier les epics sur ce board. Pour ce faire, il faut cliquer sur la liste dÃ©roulante Add List, et ajouter les labels correspondants.

Votre board devrait ressembler maintenant Ã  cela :

![Votre Product Backlog](https://user.oc-static.com/upload/2019/05/23/15586246843437_product-board-detail.png)

Nous avons maintenant tous les outils nÃ©cessaires afin de commencer notre travail sur le projet. Le workflow de dÃ©veloppement se dÃ©roule comme suit :

1. Le product owner travaille avec les utilisateurs finaux afin de dÃ©finir un **Product Backlog**. Ce Product Backlog est constituÃ© de diffÃ©rentes **epics**.
2. Ces **epics** doivent faire apparaÃ®tre plusieurs critÃ¨res, comme par exemple des wireframes ou autres Ã©crans dÃ©finissant l'application. Une epic en high passe en sprint backlog lorsque son pÃ©rimÃ¨tre est dÃ©limitÃ© avec un label **Ready**, et le dÃ©veloppement est confirmÃ© avec le client. Ce nâ€™est quâ€™Ã  ce moment que lâ€™epic est discutÃ©e avec lâ€™Ã©quipe et dÃ©composÃ©e en user stories, dÃ©crivant la feature Ã  dÃ©velopper.
3. Les **user stories** doivent respecter la Definition of Ready dÃ©finie plus tÃ´t, lors du cycle de dÃ©veloppement. Une user story n'est ready que si elle a bien Ã©tÃ© Ã©crite, et contient tous les Ã©lÃ©ments nÃ©cessaires Ã  son dÃ©veloppement durant le sprint.
4. Une fois que le sprint backlog a Ã©tÃ© dÃ©composÃ© en user stories, nous pouvons alors commencer la planification du sprint backlog.

Pour ce cours, nous allons crÃ©er **deux issues** :
- **une epic** ;
- **une user story**.

### CrÃ©ez votre premiÃ¨re epic

Pour crÃ©er une **epic**, le plus simple est d'aller dans le sous-menu List du menu Issues, et de cliquer sur le bouton **New Issue**.

Sur la nouvelle page, il faut alors renseigner les champs nÃ©cessaires. CommenÃ§ons par renseigner le titre, ainsi que la description de l'issue :

**Titre de l'issue :**

```
[EPIC] Gestion des vÃ©tÃ©rinaires
```

**Description de l'issue :**

```
Dans le projet *PetClinic*, il faut ajouter la gestion des vÃ©tÃ©rinaires.
ImplÃ©mentation de :
- CrÃ©ation des vÃ©tÃ©rinaires
- Recherche des vÃ©tÃ©rinaires
- Mise Ã  jour des vÃ©tÃ©rinaires
- Suppression des vÃ©tÃ©rinaires

TÃ¢ches Ã  faire :
- [x] CrÃ©ation des vÃ©tÃ©rinaires
- [x] Recherche des vÃ©tÃ©rinaires
- [x] Mise Ã  jour des vÃ©tÃ©rinaires
- [ ] Suppression des vÃ©tÃ©rinaires
- [ ] Supprimer les vÃ©tÃ©rinaires
- [x] Suppression de la base de donnÃ©es
```

Ajoutez-lui le label Epic :

![CrÃ©ez votre premiÃ¨re epic](https://user.oc-static.com/upload/2019/05/23/15586247348176_new-epic.png)

Et enregistrez en cliquant sur le bouton Submit Issue.

### CrÃ©ez votre premiÃ¨re user story

La deuxiÃ¨me issue sera de type user story. Ajoutez une nouvelle issue en cliquant sur le bouton New Issue :

**Titre de l'issue :**

```
[User story] Suppression du vÃ©tÃ©rinaire
```

**Description de l'issue :**

```
En tant qu'administrateur, je dois pouvoir supprimer un vÃ©tÃ©rinaire afin que celui-ci ne s'affiche plus dans l'application
```

Et enregistrez-la.

Si vous revenez sur la liste des issues, vous avez maintenant deux issues crÃ©Ã©es :

![Vos 2 issues crÃ©Ã©es](https://user.oc-static.com/upload/2019/05/23/15586248070332_list-issues.png)

### Organisez votre backlog

Nous allons maintenant **associer la user story** fraÃ®chement crÃ©Ã©e Ã  l'epic que l'on a crÃ©Ã©e auparavant. Pour ce faire, il suffit d'aller sur la description de l'epic, de cliquer sur le bouton +, au niveau du menu Related Issues, d'ajouter le numÃ©ro de la user story (dans mon cas, #5), et de cliquer sur Add.

Si nous retournons maintenant sur le menu Boards, nous allons voir que sur les deux boards crÃ©Ã©s (Development et Product), dans la partie Open, nous avons nos deux issues crÃ©Ã©es prÃ©cÃ©demment :

![Votre user story associÃ©e Ã  votre epic](https://user.oc-static.com/upload/2019/05/23/15586248607207_board-not-filtered.png)

Cependant, nous ne voulons voir ici que les issues associÃ©es Ã  chacun des boards (user stories pour le board Development, et epic pour le board Product). Pour ce faire, il suffit de cliquer sur le bouton Edit Board, et d'ajouter uniquement les labels qui nous intÃ©ressent :

![Configurez vos boards](https://user.oc-static.com/upload/2019/05/23/15586249458233_edit-board-development.png)

En modifiant les boards, nous nous retrouvons bien avec les issues qui nous intÃ©ressent sur chacun des boards.

En dÃ©plaÃ§ant nos issues sur chacun des boards, nous pouvons jouer sur la priorisation des epics, ou l'avancement des user stories.

![Priorisez vos epics](https://user.oc-static.com/upload/2019/05/23/15586250482371_priorizing-product.png)

La derniÃ¨re chose Ã  faire est de crÃ©er un sprint, et de voir le burndown chart associÃ©. Pour ce faire, il faut aller dans le menu Milestones, et cliquer sur le bouton New Milestone.

Ensuite, il suffit de remplir les informations du sprint, et de cliquer sur le bouton Create Milestone :

![CrÃ©ez une milestone](https://user.oc-static.com/upload/2019/05/23/15586251686628_new-milestone-detail.png)

Nous avons maintenant accÃ¨s au burndown chart du sprint en cours :

![Votre burndown chart](https://user.oc-static.com/upload/2019/05/23/15586252007218_burndown-chart.png)

Afin **d'associer des issues au sprint en cours**, il suffit d'aller sur chacune des issues que l'on souhaite associer au sprint, et de modifier leur propriÃ©tÃ© sur le menu Ã  droite. Dans cet exemple, j'ai modifiÃ© le milestone, le time tracking (en commentant l'issue avec les commandes /estimate et /spend), la due date, le weight, et je me suis assignÃ© l'issue :

![Timeline d'Ã©volution des boards](https://user.oc-static.com/upload/2019/05/23/15586252464075_edit-issue.png)

DorÃ©navant, toute l'Ã©quipe pourra suivre l'avancement du burndown chart facilement :

![Burndown chart accessible Ã  toute l'Ã©quipe !](https://user.oc-static.com/upload/2019/05/23/15586252982251_burndown-chart-complete.png)

## IntÃ©grez votre code en continu

### Clonez le projet sur votre poste

Pour pouvoir commencer Ã  travailler sur le projet, afin de rÃ©cupÃ©rer le code source, nous allons dans un premier temps **cloner le repository Git**. Pour ce faire, retournez sur la **page d'accueil du projet** oÃ¹ se trouvent toutes les instructions nÃ©cessaires au clonage du projet.

Cliquez en haut Ã  droite sur le bouton bleu **Clone**, et copiez la deuxiÃ¨me ligne de la pop-up **Clone with HTTPS**.

Prenez ensuite une console d'ordinateur, reprÃ©sentant votre poste de dÃ©veloppeur, et clonez le repository en tapant la commande suivante :

```
git clone https://gitlab.com/[votre-nom-d-utilisateur]/spring-petclinic-microservices.git
cd spring-petclinic-microservices
```

Une fois le clone fait, nous nous retrouvons avec un dossier vide oÃ¹ la branche `master` est associÃ©e Ã  notre repository GitLab. Nous allons **ajouter une branche** `upstream` sur GitHub et puller les derniÃ¨res modifications GitHub. Les commandes Ã  taper sont les suivantes :

```
git remote add upstream https://github.com/spring-petclinic/spring-petclinic-microservices.git
git pull upstream master
git push origin master
```

Normalement, si les commandes prÃ©cÃ©dentes ont Ã©tÃ© exÃ©cutÃ©es, le dossier Git devrait contenir le code source du projet PetClinic, ainsi que toutes les modifications associÃ©es.

![Votre projet GitLab une fois le code clonÃ©](https://user.oc-static.com/upload/2019/05/27/15589768409364_gitlab.png)

### Activez l'intÃ©gration continue sur votre projet avec GitLab

Nous allons maintenant ajouter un **pipeline d'intÃ©gration continue**, afin d'implÃ©menter les diffÃ©rentes Ã©tapes que nous avons vues prÃ©cÃ©demment. Les Ã©tapes de ce pipeline seront lancÃ©es successivement, lors de chaque nouveau push du code sur le repo. Voici Ã  quoi ressemblera le pipeline :

![Les Ã©tapes que nous allons mettre en place](https://user.oc-static.com/upload/2019/05/29/15591251962731_1c3_illus_WHITEBG%20%280-00-17-23%29_0.png)

Pour **activer l'intÃ©gration continue sur GitLab**, le plus simple est de cliquer sur le bouton Set up CI/CD sur la page d'accueil du projet. Cette commande va crÃ©er le fichier `gitlab-ci.yml` dans votre projet. C'est sur ce fichier que vous dÃ©crirez tout votre pipeline CI/CD avec la syntaxe YAML.

Et d'ajouter les lignes suivantes dans le fichier :
```yml
stages:
  - build
  - test

cache:
  paths:
    - .m2/repository
  key: "$CI_JOB_NAME"

build_job:
  stage: build
  script:
    - ./mvnw compile
      -Dhttps.protocols=TLSv1.2
      -Dmaven.repo.local=$CI_PROJECT_DIR/.m2/repository
      -Dorg.slf4j.simpleLogger.log.org.apache.maven.cli.transfer.Slf4jMavenTransferListener=WARN
      -Dorg.slf4j.simpleLogger.showDateTime=true
      -Djava.awt.headless=true
      --batch-mode --errors --fail-at-end --show-version -DinstallAtEnd=true -DdeployAtEnd=true
  image: openjdk:8-alpine

test_job:
  stage: test
  script:
    - ./mvnw test
      -Dhttps.protocols=TLSv1.2
      -Dmaven.repo.local=$CI_PROJECT_DIR/.m2/repository
      -Dorg.slf4j.simpleLogger.log.org.apache.maven.cli.transfer.Slf4jMavenTransferListener=WARN
      -Dorg.slf4j.simpleLogger.showDateTime=true
      -Djava.awt.headless=true
      --batch-mode --errors --fail-at-end --show-version -DinstallAtEnd=true -DdeployAtEnd=true
  image: openjdk:8-alpine
```

Ce fichier est la pierre angulaire de l'implÃ©mentation d'un pipeline dans GitLab. Ce fichier s'appelle `.gitlab-ci.yml`, et c'est ici que nous allons dÃ©finir notre pipeline. Dans cet exemple, nous avons implÃ©mentÃ© **deux Ã©tapes** :
- l'Ã©tape de compilation avec la tÃ¢che `build_job` ;
- l'Ã©tape des tests avec la tÃ¢che `test_job`.

**DÃ©couvrons le fichier bloc par bloc !**

#### DÃ©finissez les Ã©tapes du pipeline

Dans un premier temps, je dÃ©finis les **Ã©tapes** de mon pipeline avec le mot clÃ© `stages`. Ce mot clÃ© permet de dÃ©finir l'ordre des Ã©tapes. Ici, la premiÃ¨re Ã©tape va Ãªtre le `build`, et ensuite les `tests`. 

Ã‡a veut dire que les **tÃ¢ches** (ou *jobs* en anglais) associÃ©es Ã  **l'Ã©tape** `build` (ici `build_job`) vont s'exÃ©cuter en premier, puis les tÃ¢ches de l'Ã©tape `test` (ici `test_job`).

#### AccÃ©lÃ©rez les Ã©tapes avec le cache

Le deuxiÃ¨me bloc, avec le mot clÃ© `cache`, est ici utilisÃ© pour accÃ©lÃ©rer toutes nos Ã©tapes. Effectivement, dans le cas d'une compilation Java avec Maven (notre cas), cette compilation rÃ©cupÃ¨re beaucoup de dÃ©pendances et de librairies externes. Ces librairies sont stockÃ©es dans le rÃ©pertoire `.m2`.

GrÃ¢ce Ã  l'utilisation du mot clÃ© `cache` et de la variable prÃ©dÃ©finie de GitLab `$CI_JOB_NAME`, ce rÃ©pertoire est commun Ã  tous les jobs du pipeline.

#### DÃ©finissez les jobs Ã  effectuer

Ensuite, je dÃ©clare deux jobs, correspondant chacun Ã  une des Ã©tapes de notre pipeline d'intÃ©gration continue. Dans ces deux jobs, nous voyons que nous avons trois diffÃ©rentes lignes. DÃ©couvrons Ã  quoi ces lignes correspondent :
- `stage` : c'est le nom de l'Ã©tape qui va apparaÃ®tre dans notre pipeline d'intÃ©gration continue. Cela correspond aussi au `stage` auquel sera exÃ©cutÃ© le job ;
- `script` : ce sont les lignes de script Ã  lancer afin d'exÃ©cuter l'Ã©tape. Ici, nous lanÃ§ons le script Maven suivant son lifecycle. Dans la partie - `build`, nous lanÃ§ons la compilation ; et dans la partie `test`, nous lanÃ§ons les tests de l'application. D'autres options sont dÃ©finies afin d'accÃ©lÃ©rer le temps de traitement de ces lignes. Le `script` va alors tÃ©lÃ©charger Maven, lâ€™outil de compilation, toutes les dÃ©pendances de lâ€™application, et lancer la compilation du projet ;
- `image` : c'est l'image Docker qui va Ãªtre lancÃ©e par GitLab afin d'exÃ©cuter les lignes de script que nous avons dÃ©finies. Ici, l'image `openjdk:8-alpine`, qui contient dÃ©jÃ  Java 8, va Ãªtre lancÃ©e afin de pouvoir compiler le projet. Une fois le fichier sauvegardÃ©, le pipeline de build se lance, et vous devriez voir les diffÃ©rentes Ã©tapes se lancer (ici, l'Ã©tape de build et l'Ã©tape de test).

Lors de l'Ã©tape de `test`, le pipeline va exÃ©cuter les tests unitaires dÃ©jÃ  prÃ©sents au sein du projet. L'objectif de cette Ã©tape est de s'assurer de lancer les tests Ã©crits par les dÃ©veloppeurs. **Si un seul de ces tests Ã©choue, le pipeline s'arrÃªte**.

### Lancez votre pipeline CI/CD

Pour voir le pipeline complet, il suffit de cliquer sur le sous-menu Pipelines dans le menu CI/CD.

![La page Pipeline avec toutes les exÃ©cutions du pipeline](https://user.oc-static.com/upload/2019/05/27/15589772375269_pipeline.png)

En cliquant sur le statut *running* du pipeline, nous avons plus de dÃ©tails sur ce pipeline, les jobs associÃ©s ainsi que leurs statuts.

![DÃ©tail d'exÃ©cution du pipeline](https://user.oc-static.com/upload/2019/05/27/15589773076171_pipeline-detail.png)

### Et si l'application contient un bug ?

#### Lancez votre pipeline avec un bug

> Afin de dÃ©montrer un workflow typique de CI/CD, nous allons **introduire un bug dans l'application**. ğŸ› Ainsi, nous verrons comment le pipeline de CI/CD le dÃ©tecte, et le corrige. Dans un premier temps, nous allons rÃ©cupÃ©rer le fichier que nous venons de crÃ©er :

```
git pull
```

Le dossier courant devrait avoir le nouveau fichier `.gitlab-ci.yml`. Nous allons Ã©diter un fichier afin d'introduire un bug. Dans un premier temps, crÃ©ez une nouvelle branche qui va accueillir nos modifications :

```
git checkout -b refactor-customers
```

Ensuite, **Ã©ditez le fichier** Java "*spring-petclinic-customers-service/src/main/java/org/springframework/samples/petclinic/customers/CustomersServiceApplication.java*" Supprimez par exemple le " **;** " situÃ© Ã  la fin de la ligne `package org.springframework.samples.petclinic.customers` et commitez les changements dans Git :
```
git add spring-petclinic-customers-service/src/main/java/org/springframework/samples/petclinic/customers/CustomersServiceApplication.java
git commit -m "Refactorisation du code des clients"
git push origin refactor-customers
```

Vous devriez maintenant avoir deux branches visibles sur la page d'accueil du projet.

**Puisque vous avez introduit un bug, le pipeline de build est maintenant en Ã©chec :**

![Pipeline en Ã©chec](https://user.oc-static.com/upload/2019/05/27/15589774662327_build-failed.png)


En cliquant sur la croix rouge dans la colonne Stage, nous avons le dÃ©tail de l'erreur (ici, le bug que nous avons introduit) :

![DÃ©tail de l'erreur d'exÃ©cution du pipeline](https://user.oc-static.com/upload/2019/05/27/155897750423_build-failed-detail.png)

#### GÃ©rez le fix du bug sur GitLab

Pour enregistrer le bug, nous pouvons crÃ©er un nouveau Bug directement en cliquant sur le bouton New Issue. Il faut alors remplir les champs adÃ©quats comme l'assignee, le milestone, les labels (Bug et User Story) ou la due date :

![CrÃ©ation d'une nouvelle issue Bug dans GitLab](https://user.oc-static.com/upload/2019/05/27/15589775778674_new-bug.png)

Une fois l'issue complÃ©tÃ©e, nous avons tous les dÃ©tails du bug Ã  corriger et le job en Ã©chec est automatiquement rÃ©cupÃ©rÃ© :

![Nouveau bug crÃ©Ã©](https://user.oc-static.com/upload/2019/05/27/15589777460194_new-bug-created.png)

Si nous revenons sur le board Development, nous nous apercevons que l'issue crÃ©Ã©e apparaÃ®t dans la colonne Open. Nous pouvons alors la dÃ©placer dans la colonne Doing, car nous allons la corriger.

Pour corriger ce bug automatiquement, nous allons crÃ©er une merge request, c'est-Ã -dire demander Ã  commiter les changements sur notre branche dans la branche principale `master`. Pour ce faire, il faut aller dans le menu Merge Requests et cliquer sur New Merge Request.

Dans la nouvelle page, il faut choisir la branche que nous voulons merger dans la branche principale (ici, la branche `monbug`) et cliquer sur "Compare branches and continue" :

![Comparaison des branches Master et Monbug](https://user.oc-static.com/upload/2019/05/27/15589789051477_compare-branches.png)

Au prochain Ã©cran, il faut remplir les champs de faÃ§on adÃ©quate, et cliquer sur Submit Merge Request. Pour le champ Description, la syntaxe est "Closes" et le numÃ©ro du bug, ici, le 6 :
- Title : *WIP: Mon premier bug* ;
- Description : *Closes #6* ;
- Assignee : *Assign to me* ;
- Milestone : *Sprint 1*.

![Description de mon bug](https://user.oc-static.com/upload/2019/05/27/15589790588121_submit-merge-request.png)

Une fois la merge request crÃ©Ã©e, nous avons tous les dÃ©tails de celle-ci :

![Description de mon bug](https://user.oc-static.com/upload/2019/05/27/15589790991082_merge-request-detail.png)

#### Corrigez votre bug

l est temps de corriger notre bug. Nous allons Ã©diter le fichier que nous avons modifiÃ©. Pour cela, nous allons rÃ©introduire le " **;** " manquant, puis commiter le code corrigÃ© sur la branche `monbug`.

```
git add spring-petclinic-customers-service/src/main/java/org/springframework/samples/petclinic/customers/CustomersServiceApplication.java
git commit -m "Correction du bug clients"
git push origin monbug
```

Vous allez constater qu'une fois le code pushÃ© sur la branche `monbug`, le pipeline de build se lance afin de vÃ©rifier que le code que nous avons envoyÃ© fonctionne.

Une fois que le pipeline s'est terminÃ© en succÃ¨s, nous pouvons alors enlever le statut Work in progress en cliquant sur le bouton Resolve WIP status, pour ensuite cliquer sur le bouton Merge :

![Bug rÃ©solu, mergez les branches !](https://user.oc-static.com/upload/2019/05/27/15589794618626_merge-branches.png)

Le pipeline se lance alors une derniÃ¨re fois pour vÃ©rifier que le code mergÃ© ne casse pas la compilation.

Enfin, si nous revenons sur le board Development, nous voyons que le bug est automatiquement fermÃ©, suite Ã  notre merge request.

![Les 2 premiÃ¨res Ã©tapes du pipeline sont fonctionnelles âœ…](https://user.oc-static.com/upload/2019/05/29/15591253758662_1c3_illus_WHITEBG%20%280-00-17-23%29_2.png)

> Nous avons donc rempli les 3 premiÃ¨res Ã©tapes de l'intÃ©gration continue :
> 1. âœ…Planifiez votre dÃ©veloppement.
> 2. âœ…Compilez et intÃ©grez votre code.
> 3. âœ…Testez votre code.
> 4.  Mesurez la qualitÃ© de votre code.
> 5. GÃ©rez les livrables de votre application.

## Garantissez la qualitÃ© de votre code

### Mesurez la qualitÃ© de votre code

CommenÃ§ons par **l'analyse de code statique**, afin de contrÃ´ler la **qualitÃ© du code**.

Nous allons donc modifier le pipeline de code, afin d'ajouter cette analyse de code. Il faut alors modifier le fichier `.gitlab-ci.yml` afin qu'il ressemble Ã  ceci :
```yml
stages:
  - build
  - test
  - quality

cache:
  paths:
    - .m2/repository
  key: "$CI_JOB_NAME"

build_job:
  stage: build
  script:
    - ./mvnw compile
      -Dhttps.protocols=TLSv1.2
      -Dmaven.repo.local=$CI_PROJECT_DIR/.m2/repository
      -Dorg.slf4j.simpleLogger.log.org.apache.maven.cli.transfer.Slf4jMavenTransferListener=WARN
      -Dorg.slf4j.simpleLogger.showDateTime=true
      -Djava.awt.headless=true
      --batch-mode --errors --fail-at-end --show-version -DinstallAtEnd=true -DdeployAtEnd=true
  image: openjdk:8-alpine

test_job:
  stage: test
  script:
    - ./mvnw test
      -Dhttps.protocols=TLSv1.2
      -Dmaven.repo.local=$CI_PROJECT_DIR/.m2/repository
      -Dorg.slf4j.simpleLogger.log.org.apache.maven.cli.transfer.Slf4jMavenTransferListener=WARN
      -Dorg.slf4j.simpleLogger.showDateTime=true
      -Djava.awt.headless=true
      --batch-mode --errors --fail-at-end --show-version -DinstallAtEnd=true -DdeployAtEnd=true
  image: openjdk:8-alpine

code_quality_job:
  stage: quality
  image: docker:stable
  allow_failure: true
  services:
    - docker:stable-dind
  script:
    - mkdir codequality-results
    - docker run
        --env CODECLIMATE_CODE="$PWD"
        --volume "$PWD":/code
        --volume /var/run/docker.sock:/var/run/docker.sock
        --volume /tmp/cc:/tmp/cc
        codeclimate/codeclimate analyze -f html > ./codequality-results/index.html
  artifacts:
    paths:
      - codequality-results/
```

> Ici, j'ai ajoutÃ© une Ã©tape supplÃ©mentaire de qualitÃ© de code. L'Ã©tape (*le stage*) `quality` est dÃ©fini dans le bloc `stages`. Cela veut dire que le pipeline va ajouter un job de qualitÃ© Ã  la suite de la compilation et des tests. Ce job, je l'ajoute Ã  la fin du fichier et il est appelÃ© `code_quality_job`.

Dans ce job, nous retrouvons les 3 lignes des Ã©tapes prÃ©cÃ©dentes, plus d'autres lignes :
- `allow_failure` : cette ligne **autorise l'Ã©chec de l'Ã©tape de qualitÃ©**. Comme ce n'est pas une Ã©tape critique, nous nous permettons d'autoriser l'Ã©chec et de laisser le pipeline continuer ;
- `services` : ce nouveau mot clÃ© de GitLab permet de **dÃ©marrer un dÃ©mon Docker**, pour que lâ€™exÃ©cution de notre programme dâ€™analyse de code puisse se faire. Cette ligne nous permet d'exÃ©cuter Docker au sein de l'image, afin d'exÃ©cuter l'analyse de code ;
- `script` : cette ligne est un peu plus compliquÃ©e que les lignes de script prÃ©cÃ©dentes. La premiÃ¨re ligne va crÃ©er un dossier  `codequality-results/` qui contiendra le rÃ©sultat de l'analyse de code. La deuxiÃ¨me ligne monte le code Ã  l'intÃ©rieur d'une image Docker via le dossier `/code`, et lance l'analyse via le programme `codequality`. Le rÃ©sultat sera exportÃ© dans le dossier `codequality-results` ;
- `artifact` : cette ligne est un prÃ©requis de GitLab si nous voulons voir notre Ã©volution de qualitÃ©. Le dossier `codequality-results/` sera stockÃ© **au sein de GitLab** afin de pouvoir voir le rÃ©sultat de l'analyse du scan. Ce rÃ©sultat sera disponible et visible au sein du job `code_quality_job`.

Tout le script est exÃ©cutÃ© au sein de l'image `docker:stable`. Cette image permet de dÃ©marrer le programme dâ€™analyse de code.

Une fois le fichier commitÃ© sous Git et envoyÃ© sur GitLab via les commandes suivantes, le pipeline d'intÃ©gration continue va alors se mettre Ã  jour et lancer une compilation, suivie d'une analyse statique du code.

Le code est alors **analysÃ© par GitLab**, et le rapport gÃ©nÃ©rÃ© stockÃ© au niveau des artefacts. Pour voir le rÃ©sultat de l'analyse de code, il suffit de naviguer dans le job `code_quality_job`, puis de cliquer sur Browse. Vous aurez alors accÃ¨s au dossier contenant le rÃ©sultat de l'analyse de code, et pourrez naviguer au sein de ce fichier, afin de voir les amÃ©liorations Ã  apporter au code.

Ã‡a y est, l'Ã©tape de qualitÃ© est implÃ©mentÃ©e !

![L'Ã©tape de qualitÃ© est implÃ©mentÃ©e âœ…](https://user.oc-static.com/upload/2019/05/29/15591255853183_1c3_illus_WHITEBG%20%280-00-17-23%29_3.png)

> D'autres outils existent pour voir la qualitÃ© d'un code de dÃ©veloppement. Le plus connu est **SonarQube**, qui permet d'afficher des rapports de qualitÃ©, l'Ã©volution de ceux-ci, ainsi qu'une dÃ©tection partielle des erreurs. 

### Packagez votre application pour la dÃ©ployer

La prochaine Ã©tape aprÃ¨s la qualitÃ© du code est le **packaging** de l'application, afin de pouvoir la dÃ©ployer plus facilement. Pour ce projet, nous allons choisir **Docker** comme programme de packaging.

> Si vous n'Ãªtes pas Ã  l'aise avec les conteneurs Docker, ils seront expliquÃ©s plus en dÃ©tail **dans la seconde partie de ce cours**, quand nous verrons la livraison continue.

GitLab vient avec une registry Docker incluse, ce qui nous permet de stocker ces images au sein de GitLab. Pour pouvoir packager nos images Docker, il est nÃ©cessaire d'ajouter une nouvelle Ã©tape Ã  notre pipeline d'intÃ©gration continue. Nous allons une nouvelle fois modifier le fichier `.gitlab-ci.yml` pour ajouter cette nouvelle Ã©tape. Le fichier final ressemblera alors Ã  ceci :

```yml
stages:
  - build
  - test
  - quality
  - package

cache:
  paths:
    - .m2/repository
  key: "$CI_JOB_NAME"

build_job:
  stage: build
  script:
    - ./mvnw compile
      -Dhttps.protocols=TLSv1.2
      -Dmaven.repo.local=$CI_PROJECT_DIR/.m2/repository
      -Dorg.slf4j.simpleLogger.log.org.apache.maven.cli.transfer.Slf4jMavenTransferListener=WARN
      -Dorg.slf4j.simpleLogger.showDateTime=true
      -Djava.awt.headless=true
      --batch-mode --errors --fail-at-end --show-version -DinstallAtEnd=true -DdeployAtEnd=true
  image: openjdk:8-alpine

test_job:
  stage: test
  script:
    - ./mvnw test
      -Dhttps.protocols=TLSv1.2
      -Dmaven.repo.local=$CI_PROJECT_DIR/.m2/repository
      -Dorg.slf4j.simpleLogger.log.org.apache.maven.cli.transfer.Slf4jMavenTransferListener=WARN
      -Dorg.slf4j.simpleLogger.showDateTime=true
      -Djava.awt.headless=true
      --batch-mode --errors --fail-at-end --show-version -DinstallAtEnd=true -DdeployAtEnd=true
  image: openjdk:8-alpine

code_quality_job:
  stage: quality
  image: docker:stable
  allow_failure: true
  services:
    - docker:stable-dind
  script:
    - mkdir codequality-results
    - docker run
        --env CODECLIMATE_CODE="$PWD"
        --volume "$PWD":/code
        --volume /var/run/docker.sock:/var/run/docker.sock
        --volume /tmp/cc:/tmp/cc
        codeclimate/codeclimate analyze -f html > ./codequality-results/index.html
  artifacts:
    paths:
      - codequality-results/

package_job:
  stage: package
  services:
    - docker:stable-dind
  variables:
    DOCKER_HOST: tcp://docker:2375
  script:
    - apk add --no-cache docker
    - docker login -u gitlab-ci-token -p $CI_JOB_TOKEN $CI_REGISTRY
    - ./mvnw install -PbuildDocker -DskipTests=true -DpushImage
      -Dhttps.protocols=TLSv1.2
      -Dmaven.repo.local=$CI_PROJECT_DIR/.m2/repository
      -Dorg.slf4j.simpleLogger.log.org.apache.maven.cli.transfer.Slf4jMavenTransferListener=WARN
      -Dorg.slf4j.simpleLogger.showDateTime=true
      -Djava.awt.headless=true
      --batch-mode --errors --fail-at-end --show-version -DinstallAtEnd=true -DdeployAtEnd=true
  image: openjdk:8-alpine
```

> J'ajoute dans le fichier la derniÃ¨re Ã©tape de mon pipeline d'intÃ©gration continue, l'Ã©tape (le *stage*) `package`, qui s'exÃ©cutera Ã  la suite de l'Ã©tape `quality`. J'ajoute ensuite le job `package_job` associÃ© Ã  cette Ã©tape de qualitÃ©.

Ce job supplÃ©mentaire compile le projet et **l'encapsule dans un conteneur**. Il est ensuite poussÃ© sur la registry de GitLab. Nous retrouvons toutes les lignes que nous avons vues prÃ©cÃ©demment. La partie `script` lance cependant quelques commandes supplÃ©mentaires :
- tout d'abord, nous **installons le client Docker** dans l'image `openjdk:8-alpine` afin de pouvoir lancer les commandes propres Ã  Docker ;
- ensuite, nous **nous connectons sur la registry interne de GitLab** afin de pouvoir pousser les images Docker de faÃ§on sÃ©curisÃ©e ;
- enfin, nous **lanÃ§ons la commande Maven** de crÃ©ation de l'image Docker.

Ce processus nous permettra, dans la livraison continue, de pouvoir **dÃ©ployer facilement** le mÃªme code sur diffÃ©rents environnements. Il sert aussi Ã  figer le code compilÃ© dans un **package immuable**. De ce fait, nous pouvons facilement redÃ©ployer le mÃªme code compilÃ© sur n'importe quel autre environnement. Cela assure que le code ne soit pas modifiÃ© entre deux environnements, et qu'un code testÃ© soit **dÃ©ployÃ© partout de la mÃªme faÃ§on**. Les images Docker ainsi packagÃ©es se retrouvent sur la page de la registry :

![Registry GitLab de votre projet, avec les packages du pipeline](https://user.oc-static.com/upload/2019/05/27/15589802871223_registry.png)

Nous avons maintenant toutes les Ã©tapes nÃ©cessaires pour l'intÃ©gration continue. Comme prÃ©vu, notre code est **compilÃ©** en continu, **testÃ©**, **analysÃ©** puis **packagÃ©**, prÃªt Ã  Ãªtre dÃ©ployÃ© sur de nouveaux environnements.

![Toutes les Ã©tapes de l'intÃ©gration continue sont implÃ©mentÃ©es âœ…](https://user.oc-static.com/upload/2019/05/29/1559125627351_1c3_illus_WHITEBG%20%280-00-17-23%29_4.png)

### Les autres outils de l'intÃ©gration continue

Vous avez donc vu tout au long de cette partie comment mettre en place **l'intÃ©gration continue avec GitLab**, mais sachez qu'il existe d'autres outils reprenant les mÃªmes concepts. Le plus connu et le plus utilisÃ© d'entre eux est [Jenkins](https://jenkins.io/). Avec cet outil, vous pouvez implÃ©menter toutes les Ã©tapes prÃ©cÃ©demment vues. De plus, Jenkins utilise maintenant un fichier de description comme GitLab, qui s'appelle [Jenkinsfile](https://jenkins.io/doc/book/pipeline/jenkinsfile/).

Le principe est strictement le mÃªme que **GitLab** : il s'agit d'un **fichier de description du pipeline** d'intÃ©gration continue qui va contenir toutes les Ã©tapes Ã  lancer, afin de garantir que le code compile et soit de qualitÃ© Ã  tout moment. Cependant, il est nÃ©cessaire d'installer Jenkins dans votre entreprise, de le configurer, de le maintenir et de le mettre Ã  jour, ce qui peut s'avÃ©rer **long et fastidieux**.

Pour ceux qui ne voudraient pas passer leur temps Ã  maintenir ce genre d'outils, il existe aussi d'autres outils en mode **Software-as-a-Service** (SaaS), oÃ¹ la maintenance et l'Ã©volution sont garanties par le fournisseur. Ces outils peuvent Ãªtre mieux adaptÃ©s pour mettre en place rapidement et sans effort un pipeline d'intÃ©gration continue. Les outils les plus connus dans cet Ã©cosystÃ¨me sont [Travis CI](https://travis-ci.org/) et [CircleCI](https://circleci.com/).

Le gros avantage de ces outils est que la **maintenance** n'est pas Ã  la charge de l'Ã©quipe, mais du **fournisseur**. De plus, ces outils peuvent se connecter automatiquement sur Github.com pour la plupart, ce qui Ã©vite aussi les configurations longues et fastidieuses des diffÃ©rents outils.

Enfin, GitHub a sorti une beta de son nouveau service Ã  destination des dÃ©veloppeurs, afin de pouvoir implÃ©menter rapidement des pipelines d'intÃ©gration continue : **GitHub Actions**. Le principe est toujours le mÃªme : un fichier `.workflow` permet de crÃ©er un pipeline, afin de compiler et dÃ©ployer du code sur n'importe quelle plateforme. L'avantage principal de GitHub Actions est que ce dernier est directement intÃ©grÃ© dans GitHub.

> Nous avons donc rempli les deux derniÃ¨res Ã©tapes de l'intÃ©gration continue :
> 1. âœ…Planifiez votre dÃ©veloppement.
> 2. âœ…Compilez et intÃ©grez votre code.
> 3. âœ…Testez votre code.
> 4. âœ…Mesurez la qualitÃ© de votre code.
> 5. âœ…GÃ©rez les livrables de votre application.

Et voilÃ  ! Vous avez mis en place toutes les Ã©tapes de l'intÃ©gration continue ! ğŸ‘ğŸ‰

**Dans la prochaine partie**, nous mettrons en place la livraison continue, du dÃ©ploiement Ã  la supervision, en passant par les tests en production.

## Exercice ! Mettez en place le pipeline CI de votre application avec GitLab

Vous Ãªtes le nouvel **ingÃ©nieur DevOps** de l'entreprise **PetClinic**. Lâ€™Ã©quipe de dÃ©veloppement se repose sur un serveur afin de compiler lâ€™application avant la livraison. Cependant, aucun test unitaire nâ€™est exÃ©cutÃ©, et les clients se plaignent de la mauvaise qualitÃ© de lâ€™application.

Votre premiÃ¨re mission est de mettre en place un pipeline dâ€™intÃ©gration continue automatisÃ© afin de compiler lâ€™application, de lancer les tests unitaires associÃ©s, et de stocker les livrables sur un serveur afin de prÃ©parer le dÃ©ploiement.

### Description

#### Objectif

Lâ€™objectif de cette activitÃ© est **dâ€™Ã©crire le pipeline dâ€™intÃ©gration continue** nÃ©cessaire Ã  lâ€™intÃ©gration du projet. Pour cette activitÃ©, nous utiliserons le projet exemple Java **PetClinic** comme vu dans ce cours, mais dans sa version non microservice.

#### DonnÃ©es

Les donnÃ©es sont le [repository GitHub du projet Petclinic](https://github.com/spring-projects/spring-petclinic). Ce repository contient tous les Ã©lÃ©ments nÃ©cessaires au fonctionnement de lâ€™application. Ces Ã©lÃ©ments sont :
- le code source ;
- les tests unitaires ;
- la documentation.

#### Instructions

Clonez le repository dans votre repository GitLab, et mettez en place un pipeline dâ€™intÃ©gration continue contenant chacune des Ã©tapes nÃ©cessaires Ã  sa bonne exÃ©cution.

Comme vu dans ce cours, ces Ã©tapes sont les 4 Ã©tapes de lâ€™intÃ©gration continue : **compilation**, **test**, **qualitÃ©** et gestion des **livrables** (package + deploy).

Afin de pouvoir livrer lâ€™application et la stocker sous GitLab, vous aurez besoin de modifier le fichier pom.xml et d'ajouter un fichier ci-settings.xml Ã  la racine de votre projet. Pour vous aider, voici les fichiers :
- le [pom.xml](https://s3-eu-west-1.amazonaws.com/course.oc-static.com/courses/2035736/activit%C3%A9/pom.xml), vous pouvez directement remplacer votre pom.xml par celui-ci ;
- le [ci-settings.xml](https://s3-eu-west-1.amazonaws.com/course.oc-static.com/courses/2035736/activit%C3%A9/ci-settings.xml) Ã  ajouter Ã  la racine de votre projet.

Ces fichiers doivent contenir le nÃ©cessaire afin de pouvoir sâ€™authentifier auprÃ¨s du repository de GitLab, ainsi quâ€™avoir les droits nÃ©cessaires pour envoyer le livrable gÃ©nÃ©rÃ©. Pour ceci, vous pouvez vous inspirer de [la page officielle de GitLab](https://docs.gitlab.com/ee/user/project/packages/maven_repository.html).

Une image Docker a Ã©tÃ© prÃ©parÃ©e pour vous afin de faciliter toutes les Ã©tapes de l'intÃ©gration continue, et contenant tous les outils nÃ©cessaires au bon dÃ©roulement des Ã©tapes.

Une fois toutes les Ã©tapes validÃ©es, **une phrase sera affichÃ©e** dans les logs de l'Ã©tape de dÃ©ploiement. C'est en voyant cette phrase que vous saurez que vous avez rÃ©ussi cette mission !

Le nom de l'image Ã  utiliser dans le `.gitlab-ci.yml` est l'image `laurentgrangeau/oc-devops:latest`

### Question

Quelle est la phrase affichÃ©e lors de l'Ã©tape de dÃ©ploiement ?

### RÃ©ponse

<details>
    <summary>Voir la rÃ©ponse</summary>
    Something small enough to escape casual notice.
</details>

---

## Qu'est-ce que la livraison continue ?

**La livraison continue** est la suite logique de l'intÃ©gration continue. Dans l'intÃ©gration continue, nous cherchons Ã  ce que le code **compile** bien, mais aussi qu'il soit **fonctionnel** en production et de **qualitÃ©**, en lanÃ§ant le plus rÃ©guliÃ¨rement possible les **tests unitaires**. Mais il existe d'autres **types** de tests, tout aussi importants, pour garantir la qualitÃ© du code. Ces tests ne peuvent cependant pas Ãªtre lancÃ©s sans avoir un environnement dÃ©ployÃ©.

> Attention, la **livraison continue** ne doit pas Ãªtre confondue avec le **dÃ©ploiement continu**, qui est la suite logique de la livraison continue. Ces deux disciplines ont comme objectif de dÃ©ployer une application en production. La diffÃ©rence se trouve dans l'automatisation du dÃ©ploiement en production. La livraison continue s'arrÃªte avant la production, et la mise en production reste un acte manuel (que ce soit avec un outil, ou automatisÃ© via un clic de bouton, ou bien manuellement). La mise en production est soumise alors Ã  la **validation d'un Ãªtre humain**. <br/><br/>
> Le **dÃ©ploiement continu**, quant Ã  lui, est l'extension de la livraison continue : le dÃ©ploiement se fait de maniÃ¨re **automatisÃ©e** par un pipeline. Toutes les Ã©tapes de compilation, tests unitaires et autres tests automatisÃ©s doivent Ãªtre alors au vert avant de procÃ©der au dÃ©ploiement.

La **livraison continue** est une discipline oÃ¹ l'application est construite de maniÃ¨re Ã  pouvoir Ãªtre mise en production Ã  n'importe quel moment.

Pour atteindre la mise en oeuvre de la livraison continue sur une application, il est nÃ©cessaire de mettre en place plusieurs Ã©tapes supplÃ©mentaires au sein de notre pipeline.

> Pour mettre en place la **livraison continue**, vous devez mettre en place **5 Ã©tapes** :
> 1. La codification de l'infrastructure avec l'**Infrastructure-as-Code**.
> 2. Le **dÃ©ploiement** de votre application.
> 3. Le **test** de votre application en environnement de test.
> 4. La **supervision** de l'application.
> 5. La mise en place de **notifications** d'alerte.

### Ã‰tape 1 : Codifiez votre infrastructure avec l'Infrastructure-as-Code

> L'**Infrastructure-as-Code** est une pratique qui consiste Ã  **dÃ©crire une infrastructure avec du code**. Ce code est alors stockÃ© avec le code de l'application, et fait partie intÃ©grante de cette derniÃ¨re.

Les avantages sont nombreux :

- possibilitÃ© de crÃ©er des environnements **Ã  la demande** ;
- **crÃ©ation d'environnement en quelques minutes**, contre plusieurs semaines dans une entreprise classique ;
- **pilotage de l'infrastructure** grÃ¢ce au pipeline de livraison continue ;
- **connaissance des logiciels** installÃ©s sur la plateforme, grÃ¢ce Ã  l'outillage ;
- **montÃ©e de version** des environnements automatisÃ©s.

> Les outils principaux de l'Infrastructure-as-Code sont **Docker**, **Chef**, **Puppet**, **Ansible** et **Terraform**.

### Ã‰tape 2 : DÃ©ployez votre application

L'Ã©tape la plus importante de la livraison continue est le **dÃ©ploiement du package** que nous avons prÃ©cÃ©demment crÃ©Ã© lors de l'intÃ©gration continue. Les avantages d'utiliser un outil pour **automatiser le dÃ©ploiement** de l'application sont nombreux :
- cela permet Ã  l'Ã©quipe de **se concentrer sur le dÃ©veloppement**, lÃ  oÃ¹ elle a sa valeur Ã  ajouter ;
- n'importe qui dans l'Ã©quipe peut dÃ©ployer des logiciels ;
- les dÃ©ploiements deviennent beaucoup moins sujets aux **erreurs** et beaucoup plus **reproductibles** ;
- dÃ©ployer sur un **nouvel environnement** est facile ;
- les dÃ©ploiements peuvent Ãªtre **trÃ¨s frÃ©quents**.

> Pour pouvoir dÃ©ployer les artefacts prÃ©cÃ©demment crÃ©Ã©s, vous pourrez utiliser **Spinnaker**, **XLDeploy** ou **UrbanCode**.

### Ã‰tape 3 : Testez votre application

C'est dans cette Ã©tape que nous allons ajouter **d'autres types de tests**, plus pertinents et plus fonctionnels, afin de garantir que l'application **fonctionne comme nous l'avons estimÃ©**.

L'avantage de tester Ã  ce stade du pipeline est que l'application tourne sur un environnement de test, **presque identique Ã  celui de la production**. Son comportement sera donc le plus fidÃ¨le possible Ã  celui qu'elle aura en production.

Ces tests peuvent Ãªtre de diffÃ©rents types :

#### Test d'acceptance

Les **tests d'acceptance** sont des tests formels exÃ©cutÃ©s pour vÃ©rifier si un systÃ¨me satisfait Ã  ses exigences opÃ©rationnelles. Ils exigent que l'application entiÃ¨re soit opÃ©rationnelle et se concentrent sur la rÃ©plication des comportements des utilisateurs. Mais ils peuvent aussi aller plus loin en mesurant la performance du systÃ¨me, et rejeter les changements si certains objectifs ne sont pas atteints.

Ces tests peuvent Ãªtre **automatisÃ©s**, mais aussi **manuels**, avec une Ã©quipe de test dÃ©diÃ©e qui regardera si le logiciel correspond au besoin.

> Pour lancer des tests d'acceptance, vous pourrez utiliser **Confluence**, **FitNesse** ou **Ranorex**.

#### Test de performance

Les **tests de performance** vÃ©rifient le comportement du systÃ¨me lorsqu'il est soumis Ã  une charge importante. Ces tests ne sont pas fonctionnels et peuvent prendre diffÃ©rentes formes pour comprendre la fiabilitÃ©, la stabilitÃ© et la disponibilitÃ© de la plateforme. Par exemple, il peut s'agir d'observer les temps de rÃ©ponse lors de l'exÃ©cution d'un grand nombre de requÃªtes, ou de voir comment le systÃ¨me se comporte avec une quantitÃ© importante de donnÃ©es.

Les tests de performance sont par nature assez coÃ»teux Ã  mettre en Å“uvre et Ã  exÃ©cuter, mais ils peuvent vous aider Ã  comprendre si de nouveaux changements vont dÃ©grader votre systÃ¨me.

> Pour faire des tests de performance, vous pourrez utiliser **JMeter**, **Apache Bench** ou **Gatling**.

#### Smoke test

Les **smoke tests** sont des tests de base qui vÃ©rifient les fonctionnalitÃ©s de base de l'application. Ils sont conÃ§us pour Ãªtre rapides Ã  exÃ©cuter, et leur but est de vous donner l'assurance que les **principales caractÃ©ristiques de votre systÃ¨me fonctionnent comme prÃ©vu**. Ils peuvent Ãªtre utiles juste aprÃ¨s une nouvelle build, pour dÃ©cider si vous pouvez ou non exÃ©cuter des tests plus coÃ»teux, ou juste aprÃ¨s un dÃ©ploiement pour s'assurer que l'application fonctionne correctement dans le nouvel environnement dÃ©ployÃ©.

Par exemple, les smoke tests peuvent s'assurer que la base de donnÃ©es rÃ©pond et est correctement configurÃ©e, mais aussi que les diffÃ©rents composants sont prÃ©sents et envoient des donnÃ©es correctes, comme des API qui devraient rÃ©pondre un code HTTP 200, ou une page web qui devrait s'afficher.

> Pour s'assurer du bon fonctionnement de l'application, vous pourrez utiliser **Selenium**, **SoapUI** ou **Cypress**.

### Ã‰tape 4 : Supervisez le comportement de votre application

Le **monitoring**, ou ***supervision***, intervient une fois que notre application est dÃ©ployÃ©e sur un environnement, que ce soit un environnement de **staging**, de **test**, de **dÃ©monstration** ou bien l'environnement de **production** lui-mÃªme.

Le principe est de **rÃ©cupÃ©rer certaines mÃ©triques** qui ont du sens pour ceux qui interviennent sur l'application. Cela peut Ãªtre par exemple le nombre de connexions HTTP, le nombre de requÃªtes Ã  la base de donnÃ©es, le temps de rÃ©ponse de certaines pages ; mais aussi des mÃ©triques plus orientÃ©es mÃ©tier, comme le chiffre d'affaires gÃ©nÃ©rÃ©, ou le nombre de personnes inscrites sur l'application.

> Pour avoir un monitoring de vos applications, vous pourrez utiliser la suite **Elastic**, **Prometheus** ou **Graylog**.

Les mÃ©triques peuvent Ãªtre aussi sur la partie livraison en elle-mÃªme, ou sur le processus de dÃ©veloppement. Par exemple, l'Ã©quipe peut mesurer le nombre de dÃ©ploiements qu'elle effectue par jour, ou encore deux autres indicateurs qui sont importants afin de voir la performance de l'Ã©quipe sur la correction d'erreurs qui peuvent survenir en production :

#### Le Mean-Time-Between-Failure

Le **Mean-Time-Between-Failure** (ou MTBF) est le temps moyen qui sÃ©pare deux erreurs en production. Plus ce temps est Ã©levÃ©, plus le systÃ¨me est stable et fiable, notamment du fait de la qualitÃ© des tests qui sont jouÃ©s lors de la livraison continue.

#### Le Mean-Time-To-Recover

Le **Mean-Time-To-Recover** (ou MTTR) est le temps moyen de correction entre deux erreurs de production. Plus ce temps est faible, plus l'Ã©quipe est apte Ã  dÃ©tecter des erreurs et Ã  les corriger rapidement.

> Des outils comme **Dynatrace**, **Sysdig** ou **New Relic** permettent d'avoir ces mÃ©triques.

### Ã‰tape 5 : Mettez en place des notifications d'alertes

La premiÃ¨re version d'une nouvelle fonctionnalitÃ© ou d'un nouveau produit ne couvre souvent pas entiÃ¨rement les besoins des clients. MÃªme lorsque l'Ã©quipe passe des semaines ou des mois Ã  construire quelque chose, le produit final est souvent vouÃ© Ã  manquer des fonctionnalitÃ©s importantes. C'est le principe du **Minimum Viable Product** (MVP) en Agile.

Il arrive donc trÃ¨s souvent de livrer des logiciels **incomplets ou buggÃ©s**, si l'Ã©quipe veut aller assez vite. Au lieu de vouloir Ã©viter cela, il est nÃ©cessaire d'adopter l'idÃ©e de **livrer des petites piÃ¨ces de valeur**.

En livrant **plus vite**, nous pouvons rÃ©parer les bugs **tant que les livraisons restent petites**, et que nous savons ce qui a Ã©tÃ© modifiÃ© dans l'application. Quand les dÃ©veloppements grossissent, ils deviennent plus difficiles Ã  gÃ©rer et Ã  remanier. Un feedback rapide, grÃ¢ce aux **tests en production** et au **monitoring**, permet d'intervenir et de corriger le problÃ¨me dÃ¨s que possible. Il nous permet d'apprendre des clients, et des erreurs, au bon moment.

Une fois le dÃ©ploiement fini et les diffÃ©rents tests effectuÃ©s, il est nÃ©cessaire d'avoir un feedback rapide de l'utilisation du logiciel. En effet, si le dÃ©ploiement de la nouvelle version du logiciel apporte des bugs malgrÃ© les diffÃ©rents tests effectuÃ©s, il faut alors les dÃ©tecter le plus rapidement possible, afin de pouvoir proposer une nouvelle correction au logiciel.

> Pour avoir un feedback rapide de vos dÃ©ploiements, vous pourrez tout simplement utiliser **Slack**, **Trello** ou **Twitter**. 

## Codifiez votre infrastructure

### Construisez les images de votre application avec Docker

Notre application PetClinic est construite Ã  partir de fichiers, nommÃ©s `dockerfiles`, dÃ©jÃ  prÃ©sents dans le contrÃ´le de code source. Ces `dockerfiles` sont prÃ©sents dans le rÃ©pertoire `Docker` du projet et contiennent les lignes suivantes :
```Dockerfile
FROM openjdk:8-jre-alpine

VOLUME /tmp

ARG DOCKERIZE_VERSION
ARG ARTIFACT_NAME
ARG EXPOSED_PORT

ENV SPRING_PROFILES_ACTIVE docker

ADD https://github.com/jwilder/dockerize/releases/download/${DOCKERIZE_VERSION}/dockerize-alpine-linux-amd64-${DOCKERIZE_VERSION}.tar.gz dockerize.tar.gz
RUN tar xzf dockerize.tar.gz
RUN chmod +x dockerize

ADD ${ARTIFACT_NAME}.jar /app.jar

RUN touch /app.jar

EXPOSE ${EXPOSED_PORT}

ENTRYPOINT ["java", "-XX:+UnlockExperimentalVMOptions", "-XX:+UseCGroupMemoryLimitForHeap", "-Djava.security.egd=file:/dev/./urandom","-jar","/app.jar"]
```

Ce `dockerfile` part d'une image qui contient dÃ©jÃ  **la version 8 de Java** et copie tous les .jar des diffÃ©rents projets, afin de construire les diffÃ©rentes images associÃ©es. Enfin, la derniÃ¨re Ã©tape de ce `dockerfile` est de lancer la commande `Java` avec le jar associÃ©.

Dans cette partie du cours, vous allez modifier le `dockerfile` pour voir l'impact de l'Infrastructure-as-Code sur votre pipeline de dÃ©ploiement. Pour ce faire, ouvrez le fichier `Dockerfile` prÃ©sent dans le dossier Docker, afin de modifier la version du runtime de Java en version `openjdk:13-alpine` :

![Changez la version de l'openjdk dans le dockerfile](https://user.oc-static.com/upload/2019/05/23/15586191895254_change-openjdk.png)

Avant de pousser ce fichier sur Git, vous allez modifier la version de release de chaque `pom.xml` prÃ©sent dans le projet, pour incrÃ©menter le numÃ©ro de version des images Docker crÃ©Ã©es, et ainsi ne pas Ã©craser les versions prÃ©cÃ©demment buildÃ©es :

![Changez la version des images Docker crÃ©Ã©es dans les fichiers pom.xml](https://user.oc-static.com/upload/2019/05/23/15586192536832_modify-pom.png)

**Il y a plusieurs `pom.xml` oÃ¹ il faut ajouter le bon numÃ©ro de version !**

Une fois ces modifications faites, poussez les fichiers sur Git :
```bash
git add .
git commit -m "Modification de la version de Java et incrÃ©mentation du numÃ©ro de version"
git push origin master
```

Le pipeline d'intÃ©gration continue devrait se lancer :

![Le pipeline se lance Ã  nouveau](https://user.oc-static.com/upload/2019/05/23/15586193101822_pipeline-launch.png)

Et la registry Docker devrait contenir les nouvelles images buildÃ©es grÃ¢ce au pipeline.

Vous venez de voir Ã  quel point l'**Infrastructure-as-Code** est pratique pour tester rapidement le changement de version d'un framework, ou le changement de version d'un middleware comme Apache ou IIS. En ne changeant que quelques lignes, nous pouvons alors relancer tout le pipeline, afin de voir s'il y a un impact sur le code applicatif.

### DÃ©ployez votre application avec Docker Compose

L'Infrastructure-as-Code ne s'arrÃªte pas lÃ . Dans le cas de Docker, toute l'application peut Ãªtre dÃ©ployÃ©e grÃ¢ce au fichier `docker-compose.yml` qui contient toute la dÃ©finition de l'application, la relation entre les images Docker et le sens de dÃ©marrage de celles-ci.

Le fichier `docker-compose.yml` prÃ©sent dans le repository Git dÃ©finit des images en dur. Vous allez **remplacer le nom des images par les nouvelles images que vous venez de crÃ©er**.

Remplacez alors toutes les lignes contenant `mszarlinski/` par votre nom de registry (chez moi, `registry.gitlab.com/laurentgrangeau/`). De plus, ajoutez aussi en bout de ligne le numÃ©ro de version de l'image que vous venez de crÃ©er `:2.0.7`

Le fichier `docker-compose.yml` devrait ressembler Ã  ceci (le fichier est volontairement tronquÃ©) :

```yml
version: '2'

volumes:
  graf-data:

services:
  config-server:
    image: registry.gitlab.com/laurentgrangeau/spring-petclinic-microservices/spring-petclinic-config-server:2.0.7
    container_name: config-server
    mem_limit: 512M
    ports:
     - 8888:8888

  discovery-server:
    image: registry.gitlab.com/laurentgrangeau/spring-petclinic-microservices/spring-petclinic-discovery-server:2.0.7
    container_name: discovery-server
    mem_limit: 512M
    depends_on:
      - config-server
    entrypoint: ["./dockerize","-wait=tcp://config-server:8888","-timeout=120s","--","java", "-XX:+UnlockExperimentalVMOptions", "-XX:+UseCGroupMemoryLimitForHeap", "-Djava.security.egd=file:/dev/./urandom","-jar","/app.jar"]
    ports:
     - 8761:8761

...
```

Et voilÃ , vos fichiers Docker et Docker Compose sont prÃªts Ã  Ãªtre lancÃ©s par votre pipeline de livraison continue.

> Nous avons donc rempli la premiÃ¨re Ã©tape de la livraison continue : 
> 1. âœ…La codification de l'infrastructure avec l'**Infrastructure-as-Code**.
> 2. Le **dÃ©ploiement** de votre application.
> 3. Le **test** de votre application en environnement de test.
> 4. La **supervision** de l'application.
> 5. La mise en place de **notifications** d'alerte.

## DÃ©ployez et testez votre code sur diffÃ©rents environnements

### PrÃ©parez votre environnement de travail

Afin de vous faciliter la tÃ¢che et de ne pas installer des dÃ©pendances inutiles, je vous conseille de crÃ©er un environnement sur le site [Play-With-Docker](https://labs.play-with-docker.com/). Ce site va vous permettre de crÃ©er une infrastructure Docker rapidement. Rendez-vous sur le site, et connectez-vous avec vos identifiants Docker Hub.

Une fois connectÃ©, une session de 4 heures est crÃ©Ã©e afin de vous permettre de dÃ©ployer vos images. Sur la page d'accueil, cliquez sur l'icÃ´ne ğŸ”§ et sÃ©lectionnez le template **3 Managers and 2 Workers**.

Cela va vous crÃ©er un cluster Docker Swarm, nÃ©cessaire au dÃ©ploiement des images. Une fois le cluster crÃ©Ã©, vous allez rÃ©cupÃ©rer l'URL de l'environnement. Il suffit de copier l'URL prÃ©sente dans la case SSH.

![Copiez l'URL SSH de votre Docker Swarm sur le Play-With-Docker](https://user.oc-static.com/upload/2019/05/23/15586194362038_pwd-ssh.png)

Cette URL sera utilisÃ©e pour configurer l'environnement de dÃ©ploiement dans le fichier `.gitlab-ci.yml`. Maintenant, modifiez ce fichier pour ajouter deux nouvelles lignes. La premiÃ¨re ligne Ã  ajouter est au niveau de `variables`. Cette nouvelle variable va contenir l'URL copiÃ©e prÃ©cÃ©demment (`ip172-18-0-51-bihm1906chi000b37l6g` chez moi) :

```yml
PWD: ip172-18-0-51-bihm1906chi000b37l6g
```

La deuxiÃ¨me ligne est Ã  ajouter juste aprÃ¨s l'Ã©tape `package`. Cette Ã©tape supplÃ©mentaire sera le dÃ©ploiement des images sur un environnement de staging :

```yml
deploy_staging_job:
  stage: deploy
  image: docker:stable
  script:
    - apk add --no-cache openssh-client py-pip python-dev libffi-dev openssl-dev gcc libc-dev make
    - pip install docker-compose
    - export DOCKER_HOST=tcp://$PLAYWD.direct.labs.play-with-docker.com:2375
    - docker-compose down
    - docker-compose up -d
  environment:
    name: staging
    url: http://$PLAYWD-8080.direct.labs.play-with-docker.com
```

La syntaxe est la mÃªme que les prÃ©cÃ©dentes Ã©tapes. Dans la partie script, nous avons ajoutÃ© la copie du fichier `docker-compose.yml`, ainsi que le dossier `docker`. Enfin, nous dÃ©marrons le projet grÃ¢ce Ã  Docker Compose.

Si tout s'est bien passÃ©, vous devriez voir apparaÃ®tre dans vos environnements (*OpÃ©rations > Environnements*), le nouvel environnement **Staging**.

![Votre nouvel environnement de staging](https://user.oc-static.com/upload/2019/05/23/15586195611102_env-staging.png)

Vous pouvez alors cliquer sur le lien "*Open live environment*" sur la droite de cet environnement, afin de voir l'application dÃ©ployÃ©e.

![Votre application dÃ©ployÃ©e](https://user.oc-static.com/upload/2019/05/23/15586196079905_petclinic.png)

Maintenant que l'environnement **Staging** est dÃ©ployÃ©, il est possible de lancer des tests impossibles Ã  lancer lors de la phase d'intÃ©gration continue. Dans ce cours, nous allons lancer un test de performance, afin de mesurer les temps de rÃ©ponse de l'application. Pour ce faire, vous allez utiliser Apache Benchmark pour simuler de la charge sur le serveur.

Il faut alors ajouter de nouvelles lignes dans le fichier `.gitlab-ci.yml`, afin de lancer les tests de performance sur le nouvel environnement :

```yml
performance_job:
  stage: performance
  image: docker:git
  variables:
    URL: http://$PLAYWD-8080.direct.labs.play-with-docker.com/
  services:
    - docker:stable-dind
  script:
    - apk add --no-cache curl
    - x=1; while [[ "$(curl -s -o /dev/null -w ''%{http_code}'' http://$PLAYWD-8080.direct.labs.play-with-docker.com/)" != "200" || $x -le 60 ]]; do sleep 5; echo $(( x++ )); done || false
    - mkdir gitlab-exporter
    - wget -O ./gitlab-exporter/index.js https://gitlab.com/gitlab-org/gl-performance/raw/master/index.js
    - mkdir sitespeed-results
    - docker run --shm-size=1g --rm -v "$(pwd)":/sitespeed.io sitespeedio/sitespeed.io:6.3.1 --plugins.add ./gitlab-exporter --outputFolder sitespeed-results $URL
    - mv sitespeed-results/data/performance.json performance.json
  artifacts:
    paths:
      - sitespeed-results/
    reports:
      performance: performance.json
```

Dans ce nouveau bloc, la syntaxe reste la mÃªme. Nous rÃ©cupÃ©rons dans un premier temps l'utilitaire de test de performance dans le bloc `script`. Nous lanÃ§ons ensuite une application qui va se charger de tester notre site et d'en extraire des mÃ©triques de performance. Ces mÃ©triques sont ensuite uploadÃ©es sur GitLab afin d'Ãªtre accessibles.

Ensuite, modifiez aussi le dÃ©but du fichier afin d'ajouter une nouvelle ligne dans le bloc `stages` :

```yml
stages:
  - build
  - test
  - quality
  - package
  - deploy
  - performance
```

Enfin, une fois l'environnement de staging dÃ©ployÃ© et testÃ©, il ne reste plus qu'Ã  dÃ©ployer l'application sur l'environnement de production. Pour cela, vous allez une nouvelle fois modifier le fichier `.gitlab-ci.yml` afin d'ajouter l'Ã©tape de mise en production :

```yml
deploy_prod_job:
  stage: deploy
  image: docker:stable
  script:
    - apk add --no-cache openssh-client py-pip python-dev libffi-dev openssl-dev gcc libc-dev make
    - pip install docker-compose
    - export DOCKER_HOST=tcp://$PLAYWD.direct.labs.play-with-docker.com:2375
    - docker-compose down
    - docker-compose up -d
  environment:
    name: prod
    url: http://$PLAYWD-8080.direct.labs.play-with-docker.com
  when: manual
```

Dans cette Ã©tape, nous ajoutons le mot clÃ© `when: manual` afin de ne dÃ©ployer en production qu'avec l'intervention d'un Ãªtre humain. La validation est requise afin de savoir s'il existe des erreurs lors du dÃ©ploiement sur **staging**. Si des erreurs existent, il n'y aura alors pas de mise en production.

Sur votre pipeline de livraison continue, le dÃ©ploiement manuel est symbolisÃ© par l'icÃ´ne â–¶ï¸ Ã  cÃ´tÃ© de l'Ã©tape `deploy_prod` :

![Le dÃ©ploiement manuel sur GitLab CI](https://user.oc-static.com/upload/2019/05/23/15586196445312_manual-deploy.png)

Ces erreurs seront analysÃ©es lors de la prochaine Ã©tape : le **monitoring**.

Enfin, une technique largement utilisÃ©e lors de l'utilisation de la livraison continue est le **Canary Release**. Le principe du **Canary Release** est le mÃªme que dans les mines de charbon. Ã€ l'Ã©poque, les mineurs de charbon qui descendait Ã  la mine plaÃ§aient un canari devant eux, au bout d'une perche dans une cage. Si le canari mourait, cela voulait dire que l'air Ã©tait non respirable et les mineurs avaient le temps de rebrousser chemin afin d'Ã©viter un sort fatal.

Le principe est le mÃªme dans le dÃ©ploiement : une partie seulement des utilisateurs vont Ãªtre redirigÃ©s vers la nouvelle version de production, et si quelque chose se passe mal, il n'y aura uniquement qu'une petite partie des utilisateurs qui sera impactÃ©e. Pour le mettre en place sur notre projet, modifiez le fichier `.gitlab-ci.yml` en ajoutant un nouveau bloc `canary` :

```yml
canary_job:
  stage: canary
  image: docker:stable
  script:
    - apk add --no-cache openssh-client py-pip python-dev libffi-dev openssl-dev gcc libc-dev make
    - pip install docker-compose
    - export DOCKER_HOST=tcp://$PLAYWD.direct.labs.play-with-docker.com:2375
    - docker-compose down
    - docker-compose up -d
  environment:
    name: prod
    url: http://$PLAYWD-8080.direct.labs.play-with-docker.com
  when: manual
  only:
    - master
```

Le principe ici est exactement le mÃªme que la production, la diffÃ©rence Ã©tant que le dÃ©ploiement en canary est dÃ©corrÃ©lÃ© de la production.

Ensuite, modifiez le dÃ©but du fichier afin que dans le bloc `stages` soit ajoutÃ©e l'Ã©tape `canary` :

```yml
stages:
  - build
  - test
  - quality
  - package
  - canary
  - deploy
  - performance
```

Nous avons maintenant un environnement qui se dÃ©ploie en parallÃ¨le de la production, et qui contient uniquement une sous-partie des utilisateurs. Cet environnement sera trÃ¨s utile afin de faire des analyses en temps rÃ©el du comportement de l'application, et voir s'il n'y a pas d'erreurs.

Nous avons maintenant un pipeline complet de livraison continue, de la compilation du projet au dÃ©ploiement sur un environnement de **staging**, une possibilitÃ© de dÃ©ploiement en production via l'intervention d'une personne de l'Ã©quipe d'ops, par exemple, et un environnement Canary qui contient un sous-ensemble des utilisateurs, afin de voir comment se comporte l'application.

> Nous avons donc rempli les Ã©tapes 2 et 3 de la livraison continue :  
> 1. âœ…La codification de l'infrastructure avec l'**Infrastructure-as-Code**.
> 2. âœ…Le **dÃ©ploiement** de votre application.
> 3. âœ…Le **test** de votre application en environnement de test.
> 4. La **supervision** de l'application.
> 5. La mise en place de **notifications** d'alerte.

## Monitorez votre application

Afin de savoir si le dÃ©ploiement d'un systÃ¨me s'est bien dÃ©roulÃ©, il est nÃ©cessaire de le monitorer, ou le **superviser**. Cela permet de **prendre des dÃ©cisions** plus rapides, comme le rollback automatique d'une application si celle-ci ne fonctionne pas.

**Les deux derniÃ¨res Ã©tapes** de notre pipeline de livraison continue sont donc le **monitoring** de l'application, afin de savoir si celle-ci fonctionne correctement, ou prÃ©sente des erreurs, ainsi que l'activation de notifications en cas de problÃ¨me, pour avoir un **feedback rapide**.

### Supervisez votre application avec Prometheus

Dans cette section, vous allez ajouter dans GitLab la rÃ©cupÃ©ration des mÃ©triques de l'application. Lors du dÃ©ploiement de l'application, des mÃ©triques sont exposÃ©es par **Prometheus**, qui est un des composants de notre stack technique.

Le serveur Prometheus est accessible sur le port 9091, auquel vous pouvez accÃ©der en cliquant sur le numÃ©ro de port 9091 qui est apparu sur le site Play-with-Docker :

![AccÃ©dez Ã  Prometheus via le Play-with-Docker](https://user.oc-static.com/upload/2019/05/23/15586223806345_pwd-prometheus.png)

Ce serveur Prothemeus rÃ©cupÃ¨re Ã©normÃ©ment de mÃ©triques de l'application, des systÃ¨mes sous-jacents, ainsi que des images Docker, comme le nombre de connexions par seconde, le nombre de connexions total, etc. Il expose aussi des mÃ©triques applicatives, comme le nombre d'animaux crÃ©Ã©s ou mis Ã  jour.

Pour rÃ©cupÃ©rer ces mÃ©triques dans GitLab, il suffit d'aller dans le menu **Settings**, **Integrations**, puis **Prometheus**.

Ensuite, il faut activer l'intÃ©gration avec Prometheus, et le configurer. Cochez la case **Active**, renseignez l'URL du serveur Prometheus ; dans mon cas : `http://ip172-18-0-8-biiabu86chi000em9j9g-9091.direct.labs.play-with-docker.com/`et sauvegardez :

![Configurez Prometheus](https://user.oc-static.com/upload/2019/05/23/15586225480542_prometheus.png)

Nous allons ensuite dÃ©finir une mÃ©trique que nous allons suivre, afin de voir si l'application Ã  un problÃ¨me. Cliquez alors sur **New Metric** et ajoutez les informations suivantes :

![Configurez une nouvelle mÃ©trique sur Prometheus](https://user.oc-static.com/upload/2019/05/23/15586226004055_new-metric.png)

GitLab va alors rÃ©cupÃ©rer la mÃ©trique `http_server_requests_seconds_count` depuis le serveur Prometheus, et l'ajouter dans sa base de donnÃ©es interne. Suite Ã  cela, nous pouvons alors voir les graphes de ces mÃ©triques dans le menu **Operations**, puis **Metrics** oÃ¹ nous avons l'Ã©volution des connexions HTTP au fur et Ã  mesure du temps :

![Observez vos mÃ©triques](https://user.oc-static.com/upload/2019/05/23/1558622656011_metrics.png)

Ces mÃ©triques sont trÃ¨s utiles pour prendre des dÃ©cisions sur le dÃ©ploiement en production. Ici, nous voyons que les connexions HTTP se font bien, et nous sommes donc confiants sur la mise en production.

D'autres types de mÃ©triques sont aussi accessibles via GitLab, afin de prendre des dÃ©cisions et voir la productivitÃ© de l'Ã©quipe.

Par exemple, lors du prÃ©cÃ©dent chapitre, vous avez dÃ©ployÃ© un environnement Canary afin d'analyser le comportement de l'application. Pour voir comment cet environnement se comporte et si celui-ci est viable, allez dans le menu **Operations**, puis **Environments**. Vous devriez voir votre nouvel environnement `canary` et voir les mÃ©triques associÃ©es :

![Les mÃ©triques associÃ©es Ã  Canary](https://user.oc-static.com/upload/2019/05/23/15586227065086_canary.png)

Pour voir la performance et la productivitÃ© de l'Ã©quipe, GitLab intÃ¨gre aussi des mÃ©triques concernant le code, les issues, ou encore le temps d'exÃ©cution des tests. Ces diffÃ©rentes mÃ©triques sont disponibles dans le menu Project, sous-menu Cycle Analytics.

Les mÃ©triques les plus intÃ©ressantes sont celles qui fournissent des indicateurs sur la vÃ©locitÃ© et la productivitÃ© de l'Ã©quipe. Par exemple, il est possible de voir **le temps entre la crÃ©ation d'une issue et sa rÃ©solution** dans la rubrique Review.

Il est possible de voir aussi le temps de dÃ©ploiement sur les diffÃ©rents environnements. Plus cette valeur est petite, plus il est facile de dÃ©ployer sur les environnements. Par exemple, dans l'exemple ci-dessous, le temps de dÃ©ploiement sur l'environnement de Staging est de 3 minutes en moyenne. Avec un temps de dÃ©ploiement aussi court, il est facile de dÃ©ployer une correction en production assez rapidement.

![MÃ©trique sur le temps de dÃ©ploiement](https://user.oc-static.com/upload/2019/05/23/1558622961915_metrics-staging.png)

Il y a aussi d'autres mÃ©triques qui existent, concernant le code. Par exemple, vous pouvez voir le nombre de commits, ainsi que les diffÃ©rents contributeurs dans le menu Repository, sous-menu Contributors.

![MÃ©triques sur les contributeurs](https://user.oc-static.com/upload/2019/05/23/15586230184054_commiters.png)

Il est aussi intÃ©ressant de voir le nombre de commits par jours, pour Ã©valuer le temps de travail de chaque dÃ©veloppeur. Cette mÃ©trique est disponible dans le mÃªme menu, sous-menu Charts.

### Mettez en place des notifications Slack

La derniÃ¨re Ã©tape est l'Ã©tape de **feedback rapide**. Cette Ã©tape est celle qui va nous permettre de faire le lien entre la **production** (ops), et les **dÃ©veloppeurs** (dev). C'est une Ã©tape qui donne de la visibilitÃ© aux dÃ©veloppeurs sur des problÃ¨mes qu'il peut y avoir en production. **Plus rapide est la dÃ©tection des problÃ¨mes, plus rapide est leur correction**. 

Cette Ã©tape est le lien final qui permet d'avoir notre amÃ©lioration continue durant tout le cycle de vie de l'application. GitLab permet l'intÃ©gration avec beaucoup d'applications tierces. L'intÃ©gration la plus simple est l'intÃ©gration email. Afin d'intÃ©grer GitLab avec l'email, allez dans le menu Settings, sous-menu Integrations. Dans ce menu, choisissez "Email on push". Sur le prochain Ã©cran, cochez la case Active, renseignez votre mail et cliquez sur "Test settings and save changes".

![Activez les notifications](https://user.oc-static.com/upload/2019/05/23/15586230769625_integration-mail.png)

Vous allez maintenant Ãªtre alertÃ© des diffÃ©rents commits qu'il pourrait y avoir sur le contrÃ´le de code source. Mais l'intÃ©gration n'est que partielle avec l'email. Le mieux est d'intÃ©grer un outil comme Slack qui prendra toutes les notifications de GitLab, et les affichera dans un channel dÃ©diÃ© Ã  votre application. Pour intÃ©grer Slack, il suffit d'aller dans le menu Integrations, et de choisir Slack Notifications.

![Activez les notifications Slack](https://user.oc-static.com/upload/2019/05/23/15586231586449_integration-slack.png)

De cette page, vous allez Ãªtre invitÃ© Ã  crÃ©er un webhook Slack afin de l'intÃ©grer dans votre repository GitLab.

Choisissez un channel dÃ©diÃ© Ã  votre application afin de recevoir tous les messages associÃ©s. Attention, les messages sont nombreux. Je vous conseille de ne pas l'ajouter dans le channel GÃ©nÃ©ral.

![Configurez le channel Slack oÃ¹ recevoir les notifications](https://user.oc-static.com/upload/2019/05/23/15586232034197_slack-integration.png)

Une fois le webhook crÃ©Ã©, copiez l'URL dÃ©livrÃ©e par Slack afin de la coller dans GitLab.

Lorsque l'intÃ©gration avec Slack est finie, vous recevrez tous les messages des Ã©vÃ©nements GitLab dans le channel associÃ©.

![Slack avec vos notifications de GitLab](https://user.oc-static.com/upload/2019/05/23/15586232703011_slack.png)


> Nous avons donc rempli les deux derniÃ¨res Ã©tapes de la livraison continue : 
> 1. âœ…La codification de l'infrastructure avec l'**Infrastructure-as-Code**.
> 2. âœ…Le **dÃ©ploiement** de votre application.
> 3. âœ…Le **test** de votre application en environnement de test.
> 4. âœ…La **supervision** de l'application.
> 5. âœ…La mise en place de **notifications** d'alerte.

---

## Annexes

- [Mettez en place l'intÃ©gration et la livraison continues avec la dÃ©marche DevOps - OpenClassroom](https://openclassrooms.com/fr/courses/2035736-mettez-en-place-lintegration-et-la-livraison-continues-avec-la-demarche-devops) ;
- [Devenir expert / experte en DevOps - LinkedIn Learning ](https://www.linkedin.com/learning/paths/devenir-expert-experte-en-devops?u=56745737) ;
- [Continuous integration vs. continuous delivery vs. continuous deployment - Atlasian](https://www.atlassian.com/continuous-delivery/principles/continuous-integration-vs-delivery-vs-deployment)
- [DevOps In 5 Minutes | What Is DevOps? - Simplilearn (Youtube)](https://www.youtube.com/watch?v=Xrgk023l4lI)

***

_RÃ©alisÃ© en Markdown avec [Dillinger](https://dillinger.io/) - Par [Nicolas Barbarisi](https://www.nicolas-barbarisi.com)_